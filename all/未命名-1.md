```mermaid
graph TD
    A[开始] --> B(初始化参数x1=3.0, x2=3.0lambda=【0,0,0】, mu=0.0r=1.0, c=0.2, epsilon=0.00005)
    B --> C[设置outer_iter=0]
     C --> D[计算x1_old = x1, x2_old = x2]
    D --> E[调用inner_optimization进行内层优化]
    E --> F[计算变量变化diff_norm = √[(x1-x1_old)²+(x2-x2_old)²]]
    F --> G[更新乘子<br>lambda[0] = max(0, lambda[0] + r*g1(x1,x2))<br>lambda[1] = max(0, lambda[1] + r*g2(x1,x2))<br>lambda[2] = max(0, lambda[2] + r*g3(x1,x2))<br>mu = mu + r*h(x1,x2)]
    G --> H[更新罚参数 r = r*c]
    H --> I[outer_iter = outer_iter + 1]
    I --> J[打印迭代信息]
    J --> K{diff_norm > epsilon 且 outer_iter < 1000?}
    K -- 是 --> D
    K -- 否 --> L[打印最终结果]
    L --> M[结束]
    
    subgraph "内层优化 inner_optimization"
        E1[初始化step_size=0.1, iter=0]
        E2{iter < max_iter}
        E3[计算梯度grad]
        E4[计算grad_norm = √(grad[0]²+grad[1]²)]
        E5{grad_norm < inner_tol?}
        E6[计算x1_new = x1 - step_size*grad[0], x2_new = x2 - step_size*grad[1]]
        E7[投影到可行域(0,6)×(0,8)]
        E8[计算obj_old和obj_new]
        E9{obj_new < obj_old?}
        E10[更新x1=x1_new, x2=x2_new, step_size=step_size*1.2]
        E11[step_size = min(step_size, 1.0)]
        E12[step_size = step_size*0.5]
        E13{step_size < 1e-10?}
        E14[iter = iter + 1]
        
        E --> E1
        E1 --> E2
        E2 -- 是 --> E3
        E3 --> E4
        E4 --> E5
        E5 -- 是 --> E14
        E5 -- 否 --> E6
        E6 --> E7
        E7 --> E8
        E8 --> E9
        E9 -- 是 --> E10
        E10 --> E11
        E11 --> E14
        E9 -- 否 --> E12
        E12 --> E13
        E13 -- 是 --> E14
        E13 -- 否 --> E6
        E14 --> E2
        E2 -- 否 --> E
    end
```